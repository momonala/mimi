import numpy as np
from keras.models import model_from_json
import tensorflow as tf
global graph,model
graph = tf.get_default_graph()

def load_model(weights_file='model.h5', model_json='model.json'):
    # load pretrained model and weights
    with open(model_json, 'r') as json_file:
        loaded_model_json = json_file.read()

    loaded_model = model_from_json(loaded_model_json)
    loaded_model.load_weights(weights_file)
    return loaded_model

def sample(preds, temperature=1.0):
    # helper function to sample an index from a probability array
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    probas = np.random.multinomial(1, preds, 1)
    return np.argmax(probas)

def generate_text(sentence, model, diversity=1.0, max_out_len=20, corpus='CaptionsClean.txt'):
    '''Given an input sentence, continue the text with a meme-like response.
    The repsosne is generated on a character level basis via LSTM. To allow 
    randomness and 'creativity', the diversity term is a weight for picking
    the next character based on a probablity distribution generated by the 
    LSTM. A higher diveristy means more creative responses, but also maybe
    jibberish... 
    
    Args:
        sentence(str) : input text to build on top of
        model(keras model, .h5) : pretrained LSTM model
        diversity(float): factor of 'randomness' injected into next response. Default=1.0
        max_out_len(int): maximum length of output string
        corpus(str): text file with corpus we trained on
    Returns
        out_text(str) : continuation of string with predicted text
    '''
    with open(corpus) as f:
        data = f.readlines()
    data  = [x.replace('\n', '').split(' - ', 1) for x in data]

    text = '|'.join([x[1] for x in data])
    maxlen = 40
    chars = sorted(list(set(text)))
    char_indices = dict((c, i) for i, c in enumerate(chars))
    indices_char = dict((i, c) for i, c in enumerate(chars))

    next_char = ''
    out_text = ''
    while next_char != '|':
        # vectorize input, predict next char, and advance one char
        x_pred = np.zeros((1, maxlen, len(chars)))
        for t, char in enumerate(sentence):
            x_pred[0, t, char_indices[char]] = 1.

        with graph.as_default():
            preds = model.predict(x_pred, verbose=0)[0]
        next_index = sample(preds, diversity)
        next_char = indices_char[next_index]
        sentence = sentence[1:] + next_char

        out_text += next_char
        
    if len(out_text) > max_out_len:
        out_text = out_text[:max_out_len]
    return out_text


#----- EXAMPLE USEAGE -------

loaded_model = load_model()
text = generate_text('hello world', loaded_model)
print(text)
